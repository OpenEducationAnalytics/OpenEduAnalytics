{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Student Attrition - Pre-Processing\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, by flattening the .json files landed in the Azure Machine Learning Data Lake Storage and adjust the primary keys to accomodate the OEA Workspace. \r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to flatten and clean the JSON tables:\r\n",
        "\r\n",
        "1. Set the workspace for where the Student Attrition tables are to be converted.\r\n",
        "2. Run process model functions, (processing test.json, train.json, predict.json, predict_proba.json, global_imp.json, and local_imp.json,) to pull them from stage1/Transactional/attrition_raw, and utilize data frame functions to flatten the original JSON structure. \r\n",
        "3. Run pre-process attrition data function to land flatten JSON's into stage1/Transactional/attrition folder where they can be ingested by the 0_main_attrition pipeline into Stage2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "108",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T03:13:29.757375Z",
              "session_start_time": "2023-07-27T03:13:29.8424443Z",
              "execution_start_time": "2023-07-27T03:13:29.9685159Z",
              "execution_finish_time": "2023-07-27T03:13:30.1268775Z",
              "spark_jobs": null,
              "parent_msg_id": "bdefc141-07af-4a29-83c0-9af89f104e95"
            },
            "text/plain": "StatementMeta(spark3p3sm, 108, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "workspace = 'dev'\r\n",
        "version = '0.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "108",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T03:13:30.1415544Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T03:13:30.9839028Z",
              "execution_finish_time": "2023-07-27T03:13:30.9841692Z",
              "spark_jobs": null,
              "parent_msg_id": "2df8f544-634d-42a5-98d3-76efd94a717b"
            },
            "text/plain": "StatementMeta(, 108, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-07-27 03:13:30,771 - OEA - INFO - Now using workspace: dev\n2023-07-27 03:13:30,772 - OEA - INFO - OEA initialized.\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "108",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T03:13:31.2196076Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T03:13:31.3682697Z",
              "execution_finish_time": "2023-07-27T03:13:31.5617326Z",
              "spark_jobs": null,
              "parent_msg_id": "8cef68be-534a-4469-b9d8-f1313e0d2355"
            },
            "text/plain": "StatementMeta(spark3p3sm, 108, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-07-27 03:13:31,352 - OEA - INFO - Now using workspace: dev\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "\r\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "98",
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T02:16:01.6402103Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T02:16:01.8386469Z",
              "execution_finish_time": "2023-07-27T02:16:02.0107677Z",
              "spark_jobs": null,
              "parent_msg_id": "0d4dcfa9-da7b-4f9b-a04d-08559c8cc51b"
            },
            "text/plain": "StatementMeta(spark3p3sm, 98, 30, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# 2) run process model data functions to flatten JSON files and prepare them for landing in Stage 1\r\n",
        "\r\n",
        "from pyspark.sql.functions import lit, concat, explode, col, collect_list, expr, monotonically_increasing_id\r\n",
        "import ast\r\n",
        "\r\n",
        "# helper functions to process raw data to tabular CSV\r\n",
        "def process_model_data(table_path, id_prefix):\r\n",
        "\r\n",
        "    df = spark.read.json(oea.to_url(table_path))\r\n",
        "\r\n",
        "    run_date = df.select(\"rundate\").collect()[0][0]\r\n",
        "\r\n",
        "    column_names = [row.columns for row in df.select(\"columns\").collect()][0]\r\n",
        "    column_names = [s.replace(\"-\", \"\").replace(\"/\", \"\") for s in column_names]\r\n",
        "\r\n",
        "    index_exploded = df.withColumn(\"index\", explode(col(\"index\")))\r\n",
        "    index_exploded = index_exploded.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    index_exploded = index_exploded.select([\"index\", \"id\"])\r\n",
        "\r\n",
        "    data_exploded =  df.select(col(\"data\"), explode(col(\"data\")))\r\n",
        "    data_exploded = data_exploded.selectExpr(\r\n",
        "        *[\"col[{}] as {}\".format(i, column_names[i]) for i in range(len(column_names))])\r\n",
        "    data_exploded = data_exploded.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "\r\n",
        "    df_flat = index_exploded.join(data_exploded, \"id\", \"outer\").drop(\"id\")\r\n",
        "    df_flat = df_flat.withColumn(\"rundate\", lit(run_date))\r\n",
        "\r\n",
        "    df_flat = df_flat.withColumnRenamed(\"index\", \"id\")\r\n",
        "    df_flat = df_flat.withColumn(\"id\", concat(lit(id_prefix), df_flat[\"id\"]))\r\n",
        "    \r\n",
        "    return df_flat\r\n",
        "\r\n",
        "\r\n",
        "def process_model_predictions(table_path):\r\n",
        "\r\n",
        "    df = spark.read.text(oea.to_url(table_path))\r\n",
        "\r\n",
        "    preds = df.select('value').collect()[0][0]\r\n",
        "    preds = ast.literal_eval(preds)\r\n",
        "    preds = [Row(index=index, value=value) for index, value in enumerate(preds)]\r\n",
        "\r\n",
        "    df_preds = spark.createDataFrame(preds)\r\n",
        "\r\n",
        "    df_preds = df_preds.withColumnRenamed(\"index\", \"id\")\r\n",
        "    df_preds = df_preds.withColumn(\"id\", concat(lit('test'), df_preds[\"id\"]))\r\n",
        "    df_preds = df_preds.withColumnRenamed(\"value\", \"prediction\")\r\n",
        "    \r\n",
        "    return df_preds\r\n",
        "\r\n",
        "\r\n",
        "def process_model_probs(table_path):\r\n",
        "\r\n",
        "    df = spark.read.text(oea.to_url(table_path))\r\n",
        "    probs = df.select('value').collect()[0][0]\r\n",
        "    probs = ast.literal_eval(probs)\r\n",
        "    df_probs = spark.createDataFrame(probs)\r\n",
        "\r\n",
        "    df_probs = df_probs.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    df_probs = df_probs.withColumn(\"id\", concat(lit('test'), df_probs[\"id\"]))\r\n",
        "\r\n",
        "    df_probs = df_probs.withColumnRenamed(\"_1\", \"attrition_prob\")\r\n",
        "    df_probs = df_probs.withColumnRenamed(\"_2\", \"retain_prob\")\r\n",
        "\r\n",
        "    df_probs = df_probs.select(\"id\", \"attrition_prob\", \"retain_prob\")\r\n",
        "    \r\n",
        "    return df_probs\r\n",
        "\r\n",
        "\r\n",
        "def process_model_global_imp(table_path):\r\n",
        "    \r\n",
        "    df_feature_imp = spark.read.json(oea.to_url(table_path))\r\n",
        "\r\n",
        "    df_feature_imp = df_feature_imp.select([\"data\", \"rundate\"])\r\n",
        "    df_feature_imp = df_feature_imp.withColumnRenamed(\"data\",\"feature_imp\")\r\n",
        "    df_feature_imp = df_feature_imp.withColumn(\"feature_imp\", explode(col(\"feature_imp\")))\r\n",
        "    df_feature_imp = df_feature_imp.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    df_feature_imp = df_feature_imp.select([\"id\",\"feature_imp\", \"rundate\"])\r\n",
        "\r\n",
        "    folders = table_path.split(\"/\")\r\n",
        "    table_path = \"/\".join(folders[:-1])\r\n",
        "    table_path = table_path+'/'+'model_features'\r\n",
        "    df_feature_names = spark.read.json(oea.to_url(table_path))\r\n",
        "\r\n",
        "    df_feature_names = df_feature_names.select(\"data\")\r\n",
        "    df_feature_names = df_feature_names.withColumnRenamed(\"data\",\"feature\")\r\n",
        "    df_feature_names = df_feature_names.withColumn(\"feature\", explode(col(\"feature\")))\r\n",
        "    df_feature_names = df_feature_names.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    df_feature_names = df_feature_names.select([\"id\",\"feature\"])\r\n",
        "\r\n",
        "    df_feature_imp = df_feature_imp.join(df_feature_names, \"id\", \"outer\")\r\n",
        "    df_feature_imp = df_feature_imp.select([\"id\",\"feature\", \"feature_imp\"])\r\n",
        "\r\n",
        "    return df_feature_imp\r\n",
        "\r\n",
        "def process_model_local_imp(table_path):\r\n",
        "\r\n",
        "    df_feature_imp = spark.read.json(oea.to_url(table_path))\r\n",
        "\r\n",
        "    df_feature_imp = df_feature_imp.select([\"data\", \"rundate\"])\r\n",
        "\r\n",
        "    run_date = df_feature_imp.select(\"rundate\").collect()[0][0]\r\n",
        "\r\n",
        "    folders = table_path.split(\"/\")\r\n",
        "    table_path = \"/\".join(folders[:-1])\r\n",
        "    table_path = table_path+'/'+'model_features'\r\n",
        "    df_feature_names = spark.read.json(oea.to_url(table_path))\r\n",
        "    column_names = [row.data for row in df_feature_names.select(\"data\").collect()][0]\r\n",
        "    column_names = [s.replace(\"-\", \"\").replace(\"/\", \"\") for s in column_names]\r\n",
        "\r\n",
        "    df_exploded_1 = df_feature_imp.select(explode(df_feature_imp.data).alias(\"data\"))\r\n",
        "    df_exploded_2 = df_exploded_1.select(explode(df_exploded_1.data).alias(\"data\"))\r\n",
        "    num_cols = len(column_names)\r\n",
        "    column_transformations = [\r\n",
        "        col(\"data\").getItem(i).alias(column_names[i]) for i in range(num_cols)\r\n",
        "    ]\r\n",
        "    df_exploded_final = df_exploded_2.select(column_transformations)\r\n",
        "\r\n",
        "    df_exploded_final = df_exploded_final.withColumn(\"rundate\", lit(run_date))\r\n",
        "    df_exploded_final = df_exploded_final.withColumn(\"id\", monotonically_increasing_id())\r\n",
        "    df_exploded_final = df_exploded_final.withColumn(\"id\", concat(lit('test'), df_exploded_final[\"id\"]))\r\n",
        "\r\n",
        "    df_exploded_final = df_exploded_final.select([\"id\", *df_exploded_final.columns[:-1]])\r\n",
        "    df_exploded_final = df_exploded_final.limit(df_exploded_final.count()//2)\r\n",
        "\r\n",
        "    return df_exploded_final\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "98",
              "statement_id": 28,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T02:12:57.8415193Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T02:12:58.0572856Z",
              "execution_finish_time": "2023-07-27T02:12:58.224214Z",
              "spark_jobs": null,
              "parent_msg_id": "c71ee64b-6b92-4119-8542-c6690cdf634b"
            },
            "text/plain": "StatementMeta(spark3p3sm, 98, 28, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3) this step pre-processing the canvas data through reading in the JSONs as records, corrects any schema discepancies and then writes out the df as a CSV in stage1\r\n",
        "# there is no data transformation happening in this step besides properly reading in the column dtypes properly\r\n",
        "\r\n",
        "def preprocess_attrition_data(tables_source):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        table_path = tables_source+'/'+item\r\n",
        "        # find the batch data type of the table\r\n",
        "        batch_type_folder = oea.get_folders(table_path)\r\n",
        "        batch_type = batch_type_folder[0]\r\n",
        "        # grab only the latest folder in stage1, used to write the JSON -> CSV to the same rundate folder timestamp\r\n",
        "        # idea is to mimic the same directory structure of tables landed in stage1\r\n",
        "        latest_dt = oea.get_latest_runtime(f'{table_path}/{batch_type}', \"rundate=%Y-%m-%d %H:%M:%S\")\r\n",
        "        if item == 'data_train':\r\n",
        "            df = process_model_data(table_path, 'train')\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        elif item == 'data_test':\r\n",
        "            df = process_model_data(table_path, 'test')\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        elif item == 'predictions_predict':\r\n",
        "            df = process_model_predictions(table_path)\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        elif item == 'predictions_predict_proba':\r\n",
        "            df = process_model_probs(table_path)\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        elif item == 'model_global_importance_values':\r\n",
        "            df = process_model_global_imp(table_path)\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        elif item == 'model_local_importance_values':\r\n",
        "            df = process_model_local_imp(table_path)\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/attrition/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "        else:\r\n",
        "            logger.info(f'no ad hoc processing needed for the Attrition {item} table.')\r\n",
        "    logger.info('Finished pre-processing Attrition tables')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "98",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T02:16:17.2256763Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T02:16:17.3917329Z",
              "execution_finish_time": "2023-07-27T02:16:38.9192219Z",
              "spark_jobs": null,
              "parent_msg_id": "3e63c6ca-f5be-43ea-94cf-4a6243b57862"
            },
            "text/plain": "StatementMeta(spark3p3sm, 98, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "data_test\n2023-07-27 01:48:27\n2023-07-27 02:16:20,184 - OEA - INFO - Pre-processed table: data_test from: stage1/Transactional/attrition_raw/v0.1/data_test\ndata_train\n2023-07-27 01:48:26\n2023-07-27 02:16:23,430 - OEA - INFO - Pre-processed table: data_train from: stage1/Transactional/attrition_raw/v0.1/data_train\nmodel_features\n2023-07-27 01:48:26\n2023-07-27 02:16:23,495 - OEA - INFO - no ad hoc processing needed for the Attrition model_features table.\nmodel_global_importance_values\n2023-07-27 01:48:27\n2023-07-27 02:16:25,917 - OEA - INFO - Pre-processed table: model_global_importance_values from: stage1/Transactional/attrition_raw/v0.1/model_global_importance_values\nmodel_local_importance_values\n2023-07-27 01:48:26\n2023-07-27 02:16:29,835 - OEA - INFO - Pre-processed table: model_local_importance_values from: stage1/Transactional/attrition_raw/v0.1/model_local_importance_values\npredictions_predict\n2023-07-27 01:48:26\n2023-07-27 02:16:36,635 - OEA - INFO - Pre-processed table: predictions_predict from: stage1/Transactional/attrition_raw/v0.1/predictions_predict\npredictions_predict_proba\n2023-07-27 01:48:26\n2023-07-27 02:16:38,664 - OEA - INFO - Pre-processed table: predictions_predict_proba from: stage1/Transactional/attrition_raw/v0.1/predictions_predict_proba\n2023-07-27 02:16:38,664 - OEA - INFO - Finished pre-processing Attrition tables\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# set the version number and pre-process the dataset\r\n",
        "preprocess_attrition_data(f'stage1/Transactional/attrition_raw/v{version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test Section\r\n",
        "\r\n",
        "Below functions were used when developing and testing the above notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "108",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T03:25:48.5759125Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T03:25:48.7528281Z",
              "execution_finish_time": "2023-07-27T03:25:50.6862497Z",
              "spark_jobs": null,
              "parent_msg_id": "a96dc187-25eb-481b-801d-4e5c56a9ef7e"
            },
            "text/plain": "StatementMeta(spark3p3sm, 108, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2023-07-27 03:25:50,239 - OEA - INFO - Database dropped: ldb_dev_s2i_attrition_v0p1\n2023-07-27 03:25:50,314 - OEA - INFO - Database dropped: ldb_dev_s2r_attrition_v0p1\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Database dropped: ldb_dev_s2r_attrition_v0p1'"
          },
          "execution_count": 35,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# used for testing\r\n",
        "\r\n",
        "#oea.rm_if_exists('stage1/Transactional/attrition')\r\n",
        "#oea.rm_if_exists('stage1/Transactional/attrition_raw')\r\n",
        "#oea.rm_if_exists('stage2/Ingested/attrition')\r\n",
        "#oea.rm_if_exists('stage2/Refined/attrition')\r\n",
        "#oea.drop_lake_db('ldb_dev_s2i_attrition_v0p1')\r\n",
        "#oea.drop_lake_db('ldb_dev_s2r_attrition_v0p1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p3sm",
              "session_id": "108",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-27T03:20:58.961651Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-27T03:20:59.1084491Z",
              "execution_finish_time": "2023-07-27T03:21:01.9614446Z",
              "spark_jobs": null,
              "parent_msg_id": "af55ae2c-d01c-4cbe-85f5-866baeb66f8a"
            },
            "text/plain": "StatementMeta(spark3p3sm, 108, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Entity Name,Attribute Name,Attribute Data Type,Pseudonymizationmodel_global_importance_values,,,\n,id,string,no-op\n,feature,string,no-op\n,feature_imp,string,no-op\ndata_train,,,\n,id,string,no-op\n,FirstGenerationinCollegeFlag,string,no-op\n,Gender,string,no-op\n,Race,string,no-op\n,HSGraduateorGED,string,no-op\n,Age_Term_Min,string,no-op\n,Age_Term_Max,string,no-op\n,Total_Terms,string,no-op\n,Entry_Type_DualEnrollment,string,no-op\n,Entry_Type_EarlyAdmission,string,no-op\n,Entry_Type_FirstTimeinCollege,string,no-op\n,Entry_Type_ReEntry,string,no-op\n,Entry_Type_Transfer,string,no-op\n,AcademicProbation,string,no-op\n,AcademicSuspension,string,no-op\n,AcademicWarning,string,no-op\n,GoodAcademicStanding,string,no-op\n,ProbationAfterSuspenDismiss,string,no-op\n,TransferedToNonBusiness,string,no-op\n,CumulativeGPA,string,no-op\n,CumulativeCreditHoursEarnedPerTerm,string,no-op\n,Blended,string,no-op\n,FullyOnline,string,no-op\n,RemoteLearning,string,no-op\n,RemoteLearningBlended,string,no-op\n,Traditional,string,no-op\n,Adjunct,string,no-op\n,Faculty,string,no-op\n,Unknown_IntructorType,string,no-op\n,PELL_Eligible,string,no-op\n,Dorm_Resident,string,no-op\n,Attrition,string,no-op\n,rundate,string,no-op\nmodel_local_importance_values,,,\n,id,string,no-op\n,FirstGenerationinCollegeFlag,string,no-op\n,Gender,string,no-op\n,Race,string,no-op\n,HSGraduateorGED,string,no-op\n,Age_Term_Min,string,no-op\n,Age_Term_Max,string,no-op\n,Total_Terms,string,no-op\n,Entry_Type_DualEnrollment,string,no-op\n,Entry_Type_EarlyAdmission,string,no-op\n,Entry_Type_FirstTimeinCollege,string,no-op\n,Entry_Type_ReEntry,string,no-op\n,Entry_Type_Transfer,string,no-op\n,AcademicProbation,string,no-op\n,AcademicSuspension,string,no-op\n,AcademicWarning,string,no-op\n,GoodAcademicStanding,string,no-op\n,ProbationAfterSuspenDismiss,string,no-op\n,TransferedToNonBusiness,string,no-op\n,CumulativeGPA,string,no-op\n,CumulativeCreditHoursEarnedPerTerm,string,no-op\n,Blended,string,no-op\n,FullyOnline,string,no-op\n,RemoteLearning,string,no-op\n,RemoteLearningBlended,string,no-op\n,Traditional,string,no-op\n,Adjunct,string,no-op\n,Faculty,string,no-op\n,Unknown_IntructorType,string,no-op\n,PELL_Eligible,string,no-op\n,Dorm_Resident,string,no-op\n,rundate,string,no-op\npredictions_predict,,,\n,id,string,no-op\n,prediction,string,no-op\npredictions_predict_proba,,,\n,id,string,no-op\n,attrition_prob,string,no-op\n,retain_prob,string,no-op\ndata_test,,,\n,id,string,no-op\n,FirstGenerationinCollegeFlag,string,no-op\n,Gender,string,no-op\n,Race,string,no-op\n,HSGraduateorGED,string,no-op\n,Age_Term_Min,string,no-op\n,Age_Term_Max,string,no-op\n,Total_Terms,string,no-op\n,Entry_Type_DualEnrollment,string,no-op\n,Entry_Type_EarlyAdmission,string,no-op\n,Entry_Type_FirstTimeinCollege,string,no-op\n,Entry_Type_ReEntry,string,no-op\n,Entry_Type_Transfer,string,no-op\n,AcademicProbation,string,no-op\n,AcademicSuspension,string,no-op\n,AcademicWarning,string,no-op\n,GoodAcademicStanding,string,no-op\n,ProbationAfterSuspenDismiss,string,no-op\n,TransferedToNonBusiness,string,no-op\n,CumulativeGPA,string,no-op\n,CumulativeCreditHoursEarnedPerTerm,string,no-op\n,Blended,string,no-op\n,FullyOnline,string,no-op\n,RemoteLearning,string,no-op\n,RemoteLearningBlended,string,no-op\n,Traditional,string,no-op\n,Adjunct,string,no-op\n,Faculty,string,no-op\n,Unknown_IntructorType,string,no-op\n,PELL_Eligible,string,no-op\n,Dorm_Resident,string,no-op\n,Attrition,string,no-op\n,rundate,string,no-op\n\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# used for testing\r\n",
        "\r\n",
        "#metadata = oea.create_metadata_from_lake_db('ldb_dev_s2i_attrition_v0p1')\r\n",
        "#print(metadata)\r\n",
        "#dlw = DataLakeWriter(oea.to_url('stage1/Transactional/attrition'))\r\n",
        "#dlw.write('metadata.csv', metadata)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}