{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ed-Fi Module Resource Generation**\n",
        "\n",
        "The Ed-Fi Module uses a bunch of resources, mainly dataflows and trigger pipelines (with invoke the data flows). In the interest of asset maintainability, we have developed this utility to generate the required resources. \n",
        "\n",
        "    Pre-requisites: Customized Ed-Fi Metadata file (CSV)\n",
        "    Please go through the Ed-Fi Schema in the Schemas section of OEA to understand about the Ed-Fi Metadata file and customizing it according to the requirements. \n",
        "\n",
        "After generating the Customized Ed-Fi Metadata file, run the EdFiResourceGenerator class. This will create 2 folders - pipeline and notebook. Both the folders, contain JSON files which needs to be committed and pushed to respective folders in your Repository which is being connected to synapse. \n",
        "\n",
        "You should be able to see the pipelines and notebooks in your synapse workspace. Publish them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "script_lines = \"\"\"parameters{\n",
        "\tentity as string ('students'),\n",
        "\tdirectory as string ('Latest'),\n",
        "\tschoolYear as string ('2017'),\n",
        "\tdistrictId as string ('255901')\n",
        "}\n",
        "source(output(\n",
        "\t),\n",
        "\tallowSchemaDrift: true,\n",
        "\tvalidateSchema: false,\n",
        "\tinferDriftedColumnTypes: true,\n",
        "\tignoreNoFilesFound: false,\n",
        "\tformat: 'json',\n",
        "\tfileSystem: 'stage1',\n",
        "\tfolderPath: (\"Landing/Transactional/{$directory}/v5.3/DistrictId={$districtId}/SchoolYear={$schoolYear}/{$entity}/Delta/\"),\n",
        "\tdocumentForm: 'documentPerLine',\n",
        "\tmode: 'read') ~> SourceJSON\n",
        "SelectColumns alterRow(upsertIf(true())) ~> AlterConditions\n",
        "DerivedColumn select(mapColumn(\n",
        "\t),\n",
        "\tskipDuplicateMapInputs: true,\n",
        "\tskipDuplicateMapOutputs: true) ~> SelectColumns\n",
        "SourceJSON derive(LastModifiedDate = currentUTC()) ~> DerivedColumn\n",
        "AlterConditions sink(allowSchemaDrift: true,\n",
        "\tvalidateSchema: false,\n",
        "\tformat: 'delta',\n",
        "\tfileSystem: 'stage2',\n",
        "\tfolderPath: (\"Standardized/Transactional/{$directory}/v5.3/General/DistrictId={$districtId}/SchoolYear={$schoolYear}/{$entity}\"),\n",
        "\tmergeSchema: false,\n",
        "\tautoCompact: false,\n",
        "\toptimizedWrite: true,\n",
        "\tvacuum: 0,\n",
        "\tdeletable:false,\n",
        "\tinsertable:false,\n",
        "\tupdateable:false,\n",
        "\tupsertable:true,\n",
        "\tkeys:['id'],\n",
        "\tumask: 0022,\n",
        "\tpreCommands: [],\n",
        "\tpostCommands: [],\n",
        "\tskipDuplicateMapInputs: true,\n",
        "\tskipDuplicateMapOutputs: true,\n",
        "\tsaveOrder: 1) ~> SinkDELTA\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "dataflow_string = \"\"\"{\n",
        "\t\"name\": \"Base_Flow\",\n",
        "\t\"properties\": {\n",
        "\t\t\"folder\": {\n",
        "\t\t\t\"name\": \"EdFi/Generated\"\n",
        "\t\t},\n",
        "\t\t\"type\": \"MappingDataFlow\",\n",
        "\t\t\"typeProperties\": {\n",
        "\t\t\t\"sources\": [\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"linkedService\": {\n",
        "\t\t\t\t\t\t\"referenceName\": \"LS_datalake\",\n",
        "\t\t\t\t\t\t\"type\": \"LinkedServiceReference\"\n",
        "\t\t\t\t\t},\n",
        "\t\t\t\t\t\"name\": \"SourceJSON\"\n",
        "\t\t\t\t}\n",
        "\t\t\t],\n",
        "\t\t\t\"sinks\": [\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"linkedService\": {\n",
        "\t\t\t\t\t\t\"referenceName\": \"LS_datalake\",\n",
        "\t\t\t\t\t\t\"type\": \"LinkedServiceReference\"\n",
        "\t\t\t\t\t},\n",
        "\t\t\t\t\t\"name\": \"SinkDELTA\"\n",
        "\t\t\t\t}\n",
        "\t\t\t],\n",
        "\t\t\t\"transformations\": [\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"name\": \"AlterConditions\"\n",
        "\t\t\t\t},\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"name\": \"SelectColumns\"\n",
        "\t\t\t\t},\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"name\": \"DerivedColumn\"\n",
        "\t\t\t\t}\n",
        "\t\t\t],\n",
        "\t\t\t\"scriptLines\": []\n",
        "\t\t}\n",
        "\t}\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_string = \"\"\"{\n",
        "    \"name\": \"Ingest_academicWeeks\",\n",
        "    \"properties\": {\n",
        "        \"activities\": [\n",
        "            {\n",
        "                \"name\": \"Process Upserts to stage2\",\n",
        "                \"type\": \"ExecuteDataFlow\",\n",
        "                \"dependsOn\": [],\n",
        "                \"policy\": {\n",
        "                    \"timeout\": \"1.00:00:00\",\n",
        "                    \"retry\": 0,\n",
        "                    \"retryIntervalInSeconds\": 30,\n",
        "                    \"secureOutput\": false,\n",
        "                    \"secureInput\": false\n",
        "                },\n",
        "                \"userProperties\": [],\n",
        "                \"typeProperties\": {\n",
        "                    \"dataflow\": {\n",
        "                        \"referenceName\": \"Upsert_academicWeeks\",\n",
        "                        \"type\": \"DataFlowReference\",\n",
        "                        \"parameters\": {\n",
        "                            \"entity\": \"'academicWeeks'\",\n",
        "                            \"directory\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.Directory}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            },\n",
        "                            \"schoolYear\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.SchoolYear}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            },\n",
        "                            \"districtId\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.DistrictId}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                    \"integrationRuntime\": {\n",
        "                        \"referenceName\": \"IR-DataFlows\",\n",
        "                        \"type\": \"IntegrationRuntimeReference\"\n",
        "                    },\n",
        "                    \"traceLevel\": \"Fine\",\n",
        "                    \"continuationSettings\": {\n",
        "                        \"customizedCheckpointKey\": \"c741d7b2-35a0-409f-8457-4a02ef4ab47c\"\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Process Deletes to stage2\",\n",
        "                \"type\": \"ExecuteDataFlow\",\n",
        "                \"dependsOn\": [\n",
        "                    {\n",
        "                        \"activity\": \"Process Upserts to stage2\",\n",
        "                        \"dependencyConditions\": [\n",
        "                            \"Succeeded\"\n",
        "                        ]\n",
        "                    }\n",
        "                ],\n",
        "                \"policy\": {\n",
        "                    \"timeout\": \"1.00:00:00\",\n",
        "                    \"retry\": 0,\n",
        "                    \"retryIntervalInSeconds\": 30,\n",
        "                    \"secureOutput\": false,\n",
        "                    \"secureInput\": false\n",
        "                },\n",
        "                \"userProperties\": [],\n",
        "                \"typeProperties\": {\n",
        "                    \"dataflow\": {\n",
        "                        \"referenceName\": \"Process deletes\",\n",
        "                        \"type\": \"DataFlowReference\",\n",
        "                        \"parameters\": {\n",
        "                            \"SourceTable\": \"'schools'\",\n",
        "                            \"directory\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.Directory}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            },\n",
        "                            \"schoolYear\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.SchoolYear}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            },\n",
        "                            \"districtId\": {\n",
        "                                \"value\": \"'@{pipeline().parameters.DistrictId}'\",\n",
        "                                \"type\": \"Expression\"\n",
        "                            },\n",
        "                            \"entity\": \"'schools'\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"integrationRuntime\": {\n",
        "                        \"referenceName\": \"IR-DataFlows\",\n",
        "                        \"type\": \"IntegrationRuntimeReference\"\n",
        "                    },\n",
        "                    \"traceLevel\": \"Fine\",\n",
        "                    \"continuationSettings\": {\n",
        "                        \"customizedCheckpointKey\": \"7377e987-1348-493f-a975-dff763bb3941\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"parameters\": {\n",
        "            \"Directory\": {\n",
        "                \"type\": \"string\",\n",
        "                \"defaultValue\": \"EdFi\"\n",
        "            },\n",
        "            \"SchoolYear\": {\n",
        "                \"type\": \"string\",\n",
        "                \"defaultValue\": \"2017\"\n",
        "            },\n",
        "            \"DistrictId\": {\n",
        "                \"type\": \"string\",\n",
        "                \"defaultValue\": \"255901\"\n",
        "            }\n",
        "        },\n",
        "        \"folder\": {\n",
        "            \"name\": \"EdFi/Single District Per Instance/Generated\"\n",
        "        },\n",
        "        \"annotations\": []\n",
        "    }\n",
        "}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from asyncio.windows_utils import pipe\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\\\n",
        "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\\\n",
        "        .appName('MDESparkApp')\\\n",
        "        .getOrCreate()\n",
        "class EdFiResourceGenerator():\n",
        "    \"\"\"\n",
        "    This class generates Data flows for transforming Ed-Fi tables in Azure Synapse.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    metadata_path: path to the metadata file.\n",
        "    out_path: Output path where we write all the dataflow JSON files.\n",
        "    swagger_url: URL to the Open API Swagger endpoint.\n",
        "    templates_path: Path to the Dataflow templates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metadata_path, out_path, swagger_url, templates_path=None):\n",
        "        self.metadata_path = metadata_path\n",
        "        self.out_path = out_path\n",
        "        self.templates_path = templates_path\n",
        "        self.swagger_url = swagger_url\n",
        "        self.metadata_values = spark.read.csv(self.metadata_path, header=True).collect()\n",
        "        self.tables = list(set([x.table_name for x in self.metadata_values]))\n",
        "        self.processed_tables = []\n",
        "        self.yet_to_process_tables = [x for x in self.tables]\n",
        "        self.schemas = {}\n",
        "        self.columns = {}\n",
        "        self.dependency_dict = {}\n",
        "        self.primitive_tables = []\n",
        "\n",
        "    def get_reference(self, row):\n",
        "        flattened_fields = []\n",
        "        if(row.type == 'array'):\n",
        "            reference = (row.items).split('/')[-1][:-1]\n",
        "        elif(row['$ref'] != None):\n",
        "            if('::' in row['$ref']):\n",
        "                reference = row['$ref'].split('::')[0]\n",
        "                flattened_fields = (row['$ref'].split('::')[1]).split(':')\n",
        "            else:\n",
        "                reference = row['$ref']\n",
        "        else:\n",
        "            return None, None\n",
        "        reference = reference.split('/')[-1]\n",
        "        return reference, flattened_fields\n",
        "\n",
        "    def get_data_type(self, dtype, format):\n",
        "        if(dtype == 'string'):\n",
        "            if(format == 'date'):\n",
        "                return 'date'\n",
        "            if(format == 'date-time'):\n",
        "                return 'timestamp'\n",
        "            return 'string'\n",
        "        if(dtype == 'number'):\n",
        "            return 'float'\n",
        "        return dtype\n",
        "\n",
        "    def get_schema(self, dtype, schema_list):\n",
        "        schema = ''\n",
        "        for line in schema_list:\n",
        "            line = line if line[-1] == ',' else line + ','\n",
        "            schema += line\n",
        "        schema = schema[:-1] if schema[-1] == ',' else schema\n",
        "        schema = '(' + schema + ')'\n",
        "        if(dtype == 'array'):\n",
        "            schema = schema + '[]'\n",
        "        return schema + ','\n",
        "\n",
        "\n",
        "    def create_primitive_schemas(self):\n",
        "        print(\"Creating Primitive Schemas\")\n",
        "        for table in self.tables:\n",
        "            table_values = [x for x in self.metadata_values if x.table_name == table]\n",
        "            isPrimitive = not(any(x for x in table_values if x['$ref'] != None)) and not(any(x for x in table_values if x.type == 'array'))\n",
        "            if(isPrimitive):\n",
        "                table_schema = [f'{row.column_name} as {self.get_data_type(row.type, row.format)},'  for row in table_values]\n",
        "                table_schema[-1] = table_schema[-1][:-1]\n",
        "                self.schemas[table] = table_schema\n",
        "                col_list = [row.column_name + ',' for row in table_values]\n",
        "                col_list[-1] = col_list[-1][:-1]\n",
        "                self.columns[table] = col_list\n",
        "                self.primitive_tables.append(table)\n",
        "                self.processed_tables.append(table)\n",
        "                self.yet_to_process_tables.remove(table)\n",
        "        print(\"Completed creating primitive schemas\")\n",
        "\n",
        "    def create_dependency_dict(self):\n",
        "        referenced_df_values = [x for x in self.metadata_values if x.type == 'array' or x['$ref'] != None]\n",
        "        for row in referenced_df_values:\n",
        "            reference, flatten_fields = self.get_reference(row)\n",
        "            if(row.table_name in self.dependency_dict.keys() and reference not in self.dependency_dict[row.table_name]):\n",
        "                self.dependency_dict[row.table_name].append(reference)\n",
        "            elif(row.table_name not in self.dependency_dict):\n",
        "                self.dependency_dict[row.table_name] = [reference]\n",
        "\n",
        "    def create_complex_schemas(self):\n",
        "        while len(self.yet_to_process_tables) > 0:\n",
        "            for entity in self.yet_to_process_tables:\n",
        "                if(len([x for x in self.dependency_dict[entity] if x not in self.processed_tables]) == 0):\n",
        "                    table_schema = [x for x in self.metadata_values if x.table_name == entity]\n",
        "                    spark_schema = []\n",
        "                    self.columns[entity] = []\n",
        "                    for row in table_schema:\n",
        "                        reference, flatten_fields = self.get_reference(row)\n",
        "                        if(row.type == 'array'):\n",
        "                            # Handle Array Objects\n",
        "                            column_schema = self.get_schema('array', self.schemas[reference])\n",
        "                        elif(row['$ref'] != None):\n",
        "                            # Handle Normal Objects\n",
        "                            column_schema = self.get_schema('object', self.schemas[reference])\n",
        "                        else:\n",
        "                            # Primitive Data type\n",
        "                            column_schema = self.get_data_type(row.type, row.format) + ','\n",
        "                        if(flatten_fields is None or len(flatten_fields) == 0):\n",
        "                            self.columns[row.table_name].append(row.column_name + ',')\n",
        "                        else:\n",
        "                            self.columns[row.table_name] += [f\"{x} = {row.column_name}.{x},\" for x in flatten_fields]\n",
        "                        spark_schema.append(f\"{row.column_name} as {column_schema}\")\n",
        "                    # self.columns[row.table_name][-1] = self.columns[row.table_name][-1][:-1]\n",
        "                    self.columns[row.table_name].append('LastModifiedDate')\n",
        "                    spark_schema[-1] = spark_schema[-1][:-1]\n",
        "                    self.schemas[entity] = spark_schema\n",
        "                    self.processed_tables.append(entity)\n",
        "                    self.yet_to_process_tables.remove(entity)\n",
        "            #print(len(self.yet_to_process_tables))\n",
        "\n",
        "    def create_resources(self):\n",
        "        lines = []\n",
        "        if(self.templates_path is not None):\n",
        "            with open(f'{self.templates_path}/Scriptlines.txt') as f:\n",
        "                for x in f:\n",
        "                    lines.append(x.replace('\\t', '').replace('\\n', ''))\n",
        "            with open(f\"{self.templates_path}/Dataflow.json\") as f:\n",
        "                dataflow_json = json.load(f)\n",
        "        else:\n",
        "            lines = [x.replace('\\t', '') for x in script_lines.split('\\n')]\n",
        "            dataflow_json = json.loads(dataflow_string)\n",
        "            pipeline_json = json.loads(pipeline_string)\n",
        "        swagger_json = json.loads(requests.get(self.swagger_url).text)\n",
        "\n",
        "\n",
        "        for endpoint in swagger_json['paths']:\n",
        "            if(endpoint.count('/') == 2):\n",
        "                response = swagger_json['paths'][endpoint]['get']['responses']['200']['schema']\n",
        "                reference = response['items']['$ref']\n",
        "                reference = reference.split('_')[-1]\n",
        "                entity = endpoint.split('/')[-1]\n",
        "                lines[1] = f\"entity as string ('{entity}'),\"\n",
        "                dataflow_json['properties']['typeProperties']['scriptLines'] = lines[:7] + self.schemas[reference] + lines[7:19] + self.columns[reference] + lines[19:]\n",
        "                dataflow_json['name'] = f'Upsert_{entity}'\n",
        "\n",
        "                pipeline_json['name'] = f'Ingest_{entity}'\n",
        "                pipeline_json['properties']['activities'][0]['typeProperties']['dataflow']['referenceName'] = f\"Upsert_{entity}\"\n",
        "                pipeline_json['properties']['activities'][0]['typeProperties']['dataflow']['parameters']['entity'] = f\"'{entity}'\"\n",
        "                pipeline_json['properties']['activities'][0]['typeProperties']['continuationSettings']['customizedCheckpointKey'] = str(uuid.uuid4())\n",
        "\n",
        "                pipeline_json['properties']['activities'][1]['typeProperties']['dataflow']['referenceName'] = \"Process deletes\"\n",
        "                pipeline_json['properties']['activities'][1]['typeProperties']['dataflow']['parameters']['entity'] = f\"'{entity}'\"\n",
        "                pipeline_json['properties']['activities'][1]['typeProperties']['dataflow']['parameters']['SourceTable'] = f\"'{entity}'\"\n",
        "                pipeline_json['properties']['activities'][1]['typeProperties']['continuationSettings']['customizedCheckpointKey'] = str(uuid.uuid4())\n",
        "                \n",
        "                pipeline_json['properties']['folder']['name'] = 'EdFi/Single District Per Instance/Generated'\n",
        "\n",
        "\n",
        "                with open(f\"{self.out_path}/dataflow/Upsert_{entity}.json\", 'w') as f:\n",
        "                    f.write(json.dumps(dataflow_json))\n",
        "\n",
        "                with open(f\"{self.out_path}/pipeline/Upsert_{entity}.json\", 'w') as f:\n",
        "                    f.write(json.dumps(pipeline_json))\n",
        "\n",
        "    def create_dataflows(self):\n",
        "        self.create_primitive_schemas()\n",
        "        self.create_dependency_dict()\n",
        "        self.create_complex_schemas()\n",
        "        self.create_resources()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('Spark_Dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "be620d6548581d4b255e22136663c988a2c6aa20c2fcc1339f2748495f9e98b2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
