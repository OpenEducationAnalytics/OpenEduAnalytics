{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%run /OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the below parameters from pipeline. \r\n",
        "directory = 'Ed-Fi'\r\n",
        "api_version = '5.2'\r\n",
        "metadata_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Ed-Fi/docs/edfi_oea_metadata.csv'\r\n",
        "workspace = 'prod'\r\n",
        "# TODO: swagger_url = 'https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.json'\r\n",
        "# KeyError exception because the 'x-Ed-Fi-explode' is not part of the standard swager.json\r\n",
        "# We should remove all references to 'x-Ed-Fi-explode' from these notebooks since we are now dynamicallly exploding arrays based on swagger datatypes\r\n",
        "swagger_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main//modules/module_catalog/Ed-Fi/docs/edfi_swagger.json'\r\n",
        "\r\n",
        "oea = OEA(workspace=workspace)\r\n",
        "oea_metadatas = oea.get_metadata_from_url(metadata_url)\r\n",
        "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
        "\r\n",
        "# TODO: Use the swagger file available from the Ed-Fi API landing page, instead of hardcoding it.\r\n",
        "# For example, https://api.edgraph.dev/edfi/v5.2/saas is the openApiMetadata endpoint will help fetch the descriptors and resources swagger.json\r\n",
        "# The base path of the api can be passed as a parameter to this notebook instead and assigned to swagger_url variable\r\n",
        "# This will also help get latest version of the swagger.json based on the Ed-Fi version and it will contain a list of all endpoints \r\n",
        "# and entity definition, including any \"extensions\" or custommizations\r\n",
        "# For example, \r\n",
        "# - Descriptors: https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.JSON\r\n",
        "# - Resources:   https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/descriptors/swagger.JSON\r\n",
        "schema_gen = OpenAPIUtil(swagger_url)\r\n",
        "schemas = schema_gen.create_spark_schemas()\r\n",
        "\r\n",
        "stage2_ingested = oea.to_url(f'stage2/Ingested/{directory}/v{api_version}')\r\n",
        "stage2_refined = oea.to_url(f'stage2/Refined/{directory}/v{api_version}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_descriptor_schema(descriptor):\r\n",
        "    fields = []\r\n",
        "    fields.append(StructField('_etag',LongType(), True))\r\n",
        "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
        "    fields.append(StructField('codeValue',StringType(), True))\r\n",
        "    fields.append(StructField('description',StringType(), True))\r\n",
        "    fields.append(StructField('id',StringType(), True))\r\n",
        "    fields.append(StructField('namespace',StringType(), True))\r\n",
        "    fields.append(StructField('shortDescription',StringType(), True))\r\n",
        "    return StructType(fields)\r\n",
        "\r\n",
        "def get_descriptor_metadata(descriptor):\r\n",
        "    return [['_etag', 'long', 'no-op'],\r\n",
        "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
        "            ['codeValue','string', 'no-op'],\r\n",
        "            ['description','string', 'no-op'],\r\n",
        "            ['id','string', 'no-op'],\r\n",
        "            ['namespace','string', 'no-op'],\r\n",
        "            ['shortDescription','string', 'no-op']]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\r\n",
        "import pyspark.sql.functions as f\r\n",
        "\r\n",
        "def has_column(df, col):\r\n",
        "    try:\r\n",
        "        df[col]\r\n",
        "        return True\r\n",
        "    except AnalysisException:\r\n",
        "        return False\r\n",
        "\r\n",
        "def modify_descriptor_value(df, col_name):\r\n",
        "    if col_name in df.columns:\r\n",
        "        # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\r\n",
        "        df = df.drop(col_name)\r\n",
        "    else:\r\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\r\n",
        "\r\n",
        "    return df\r\n",
        "\r\n",
        "def flatten_reference_col(df, target_col):\r\n",
        "    col_prefix = target_col.name.replace('Reference', '')\r\n",
        "    df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\r\n",
        "    df = df.drop(target_col.name)\r\n",
        "    return df\r\n",
        "\r\n",
        "def modify_references_and_descriptors(df, target_col):\r\n",
        "    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\r\n",
        "        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\r\n",
        "    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\r\n",
        "        df = modify_descriptor_value(df, desc_col)\r\n",
        "    return df\r\n",
        "\r\n",
        "def explode_arrays(df, target_col, schema_name, table_name):\r\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear']\r\n",
        "    child_df = df.select(cols + [target_col.name])\r\n",
        "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
        "\r\n",
        "    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\r\n",
        "    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\r\n",
        "    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\r\n",
        "    if(identity_cols is not None and len(identity_cols) > 0):\r\n",
        "        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\r\n",
        "    \r\n",
        "    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \r\n",
        "    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\r\n",
        "    # This must be done \"before\" the grand_child is exploded below\r\n",
        "    child_df = modify_references_and_descriptors(child_df, target_col)\r\n",
        "\r\n",
        "    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
        "        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\r\n",
        "        \r\n",
        "        # Modifying Reference and Descriptor columns for the grand_child array\r\n",
        "        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\r\n",
        "\r\n",
        "        # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
        "        grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
        "                .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}_{array_sub_col.name}\")\r\n",
        "\r\n",
        "    # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
        "    child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
        "        .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}\")\r\n",
        "\r\n",
        "    # Drop array column from parent entity\r\n",
        "    df = df.drop(target_col.name)\r\n",
        "    return df\r\n",
        "\r\n",
        "def transform(df, schema_name, table_name, parent_schema_name, parent_table_name):\r\n",
        "    if re.search('Descriptors$', table_name) is None:\r\n",
        "        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\r\n",
        "        target_schema = copy.deepcopy(schemas[table_name])\r\n",
        "        # Add primary key\r\n",
        "        if has_column(df, 'id'):\r\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('id')).cast(\"String\"))\r\n",
        "        else:\r\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
        "    else:\r\n",
        "        target_schema = get_descriptor_schema(table_name)\r\n",
        "        # Add primary key\r\n",
        "        if has_column(df, 'namespace') and has_column(df, 'codeValue'):\r\n",
        "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\r\n",
        "        else:\r\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
        "\r\n",
        "    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\r\n",
        "                                 .add(StructField('SchoolYear', StringType()))\\\r\n",
        "                                 .add(StructField('LastModifiedDate', TimestampType()))\r\n",
        "\r\n",
        "    for col_name in target_schema.fieldNames():\r\n",
        "        target_col = target_schema[col_name]\r\n",
        "        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\r\n",
        "        # Note: Descriptor is a String therefore is a Primitive datatype\r\n",
        "        if target_col.dataType.typeName() in primitive_datatypes:\r\n",
        "            # If it is a Descriptor\r\n",
        "            if re.search('Descriptor$', col_name) is not None:\r\n",
        "                df = modify_descriptor_value(df, col_name)\r\n",
        "            else:\r\n",
        "                if col_name in df.columns:\r\n",
        "                    # Casting columns to primitive data types\r\n",
        "                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
        "                else:\r\n",
        "                    # If Column not present in dataframe, add column with None values.\r\n",
        "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
        "        # If Complex datatype, i.e. Object, Array\r\n",
        "        else:\r\n",
        "            if col_name not in df.columns:\r\n",
        "                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
        "            else:\r\n",
        "                # Generate JSON column as a Complex Type\r\n",
        "                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\r\n",
        "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\r\n",
        "                    .drop(f\"{col_name}_json\")\r\n",
        "            \r\n",
        "            # Modify the links with surrogate keys\r\n",
        "            if re.search('Reference$', col_name) is not None:\r\n",
        "                df = flatten_reference_col(df, target_col)\r\n",
        "    \r\n",
        "            if target_col.dataType.typeName() == 'array':\r\n",
        "                df = explode_arrays(df, target_col, schema_name, table_name)\r\n",
        "        \r\n",
        "    return df\r\n",
        "\r\n",
        "#df = spark.read.format('delta').load(f\"{stage2_ingested}/ed-fi/absenceEventCategoryDescriptors\")\r\n",
        "#df = transform(df, \"ed-fi\", \"absenceEventCategoryDescriptors\", None, None)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for schema_name in [x.name for x in mssparkutils.fs.ls(stage2_ingested) if x.isDir]:\r\n",
        "    print(f\"Processing schema: {schema_name}\")\r\n",
        "    \r\n",
        "    for table_name in [y.name for y in mssparkutils.fs.ls(f\"{stage2_ingested}/{schema_name}\") if y.isDir]:\r\n",
        "        print(f\"Processing schema/table: {schema_name}/{table_name}\")\r\n",
        "\r\n",
        "        # 1. Read Delta table from Ingested Folder.\r\n",
        "\r\n",
        "        # Process each file even when it is empty. The tables will be created using on target_schema and will be available for query in SQL.\r\n",
        "        df = spark.read.format('delta').load(f\"{stage2_ingested}/{schema_name}/{table_name}\")\r\n",
        "\r\n",
        "        # 2. Transformation step\r\n",
        "        try:\r\n",
        "            df = transform(df, schema_name, table_name, None, None)\r\n",
        "        except:\r\n",
        "            print(f\"Error while Transforming {schema_name}/{table_name}\")\r\n",
        "\r\n",
        "        # 3. Pseudonymize the data using metadata.\r\n",
        "        if(re.search('Descriptors$', table_name) is None):\r\n",
        "            # Use Deep Copy otherwise the schemas object also gets modified every time oea_metadatas is modified\r\n",
        "            oea_metadata = copy.deepcopy(oea_metadatas[table_name])\r\n",
        "        else:\r\n",
        "            oea_metadata = get_descriptor_metadata(table_name)\r\n",
        "\r\n",
        "        oea_metadata += [\r\n",
        "                            ['DistrictId', 'string', 'partition-by'],\r\n",
        "                            ['SchoolYear', 'string', 'partition-by'],\r\n",
        "                            ['LastModifiedDate', 'timestamp', 'no-op']\r\n",
        "                        ]\r\n",
        "\r\n",
        "        try:\r\n",
        "            df_pseudo, df_lookup = oea.pseudonymize(df, oea_metadata)\r\n",
        "        except:\r\n",
        "            print(f\"Error while Pseudonymizing {schema_name}/{table_name}\")\r\n",
        "\r\n",
        "        # 4. Write to Refined folder (even when file is empty)\r\n",
        "        df_pseudo.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/General/{schema_name}/{table_name}\")\r\n",
        "        #if(len(df_lookup.columns) > 2):\r\n",
        "        df_lookup.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/Sensitive/{schema_name}/{table_name}\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}